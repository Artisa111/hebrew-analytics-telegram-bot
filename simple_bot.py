# -*- coding: utf-8 -*-
"""
×‘×•×˜ ×¤×©×•×˜ ×œ×‘×“×™×§×” - Simple bot for testing
"""

import logging
import os
import pandas as pd
import numpy as np
import tempfile
import shutil
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from telegram import Update, ReplyKeyboardMarkup, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, MessageHandler, CallbackQueryHandler, ContextTypes, filters
from telegram.constants import ParseMode
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, r2_score

# Setup logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° matplotlib Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¸Ğ²Ñ€Ğ¸Ñ‚Ğ°
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

class SimpleHebrewBot:
    def __init__(self, bot_token: str):
        self.application = Application.builder().token(bot_token).job_queue(None).persistence(None).build()
        self.user_data = {}  # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹
        self.setup_handlers()
    
    def setup_handlers(self):
        """×”×’×“×¨×ª handlers ×¤×©×•×˜×™×"""
        self.application.add_handler(CommandHandler("start", self.start_command))
        self.application.add_handler(CommandHandler("help", self.help_command))
        self.application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_text))
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
        self.application.add_handler(MessageHandler(filters.Document.ALL, self.handle_document))
    
    async def start_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """×¤×§×•×“×ª start ×¤×©×•×˜×”"""
        user = update.effective_user
        user_id = user.id
        
        # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
        self.user_data[user_id] = {
            'data': None,
            'file_name': None,
            'analysis_done': False
        }
        
        welcome_text = f"×‘×¨×•×š ×”×‘× {user.first_name}! ğŸ‰\n\n×× ×™ ×‘×•×˜ × ×™×ª×•×— × ×ª×•× ×™× ×‘×¢×‘×¨×™×ª.\n\nğŸ“ ×©×œ×— ×œ×™ ×§×•×‘×¥ CSV ××• Excel ×›×“×™ ×œ×”×ª×—×™×œ!"
        
        keyboard = [
            ['ğŸ“Š × ×™×ª×•×— × ×ª×•× ×™×'],
            ['ğŸ“ˆ ×ª×¨×©×™××™×'],
            ['ğŸ’¡ ×ª×•×‘× ×•×ª ×•×”××œ×¦×•×ª'],
            ['ğŸ“ ×”×¢×œ××ª ×§×•×‘×¥'],
            ['â“ ×¢×–×¨×”']
        ]
        reply_markup = ReplyKeyboardMarkup(keyboard, resize_keyboard=True)
        
        await update.message.reply_text(welcome_text, reply_markup=reply_markup)
    
    async def help_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """×¤×§×•×“×ª help ×¤×©×•×˜×”"""
        help_text = """
ğŸ“š **×¢×–×¨×” - ×‘×•×˜ × ×™×ª×•×— × ×ª×•× ×™× ×‘×¢×‘×¨×™×ª**

**×¤×§×•×“×•×ª ×–××™× ×•×ª:**
/start - ×”×ª×—×œ×ª ×©×™××•×© ×‘×‘×•×˜
/help - ×”×¦×’×ª ×¢×–×¨×” ×–×•

**×™×›×•×œ×•×ª ×”×‘×•×˜:**
â€¢ ğŸ“ ×”×¢×œ××ª ×§×‘×¦×™ CSV ×•-Excel
â€¢ ğŸ“Š × ×™×ª×•×— × ×ª×•× ×™× ××§×™×£
â€¢ ğŸ“ˆ ×™×¦×™×¨×ª ×ª×¨×©×™××™× ××§×¦×•×¢×™×™×
â€¢ ğŸ’¡ ×ª×•×‘× ×•×ª ××•×˜×•××˜×™×•×ª ×•×”××œ×¦×•×ª
â€¢ ğŸ” ×–×™×”×•×™ ×“×¤×•×¡×™× ×•×× ×•××œ×™×•×ª

**××™×š ×œ×”×©×ª××©:**
1. ×©×œ×— ×œ×™ ×§×•×‘×¥ CSV ××• Excel
2. ×‘×—×¨ "× ×™×ª×•×— × ×ª×•× ×™×" ×œ× ×™×ª×•×— ××§×™×£
3. ×‘×—×¨ "×ª×¨×©×™××™×" ×œ×™×¦×™×¨×ª ×’×¨×¤×™×
4. ×‘×—×¨ "×ª×•×‘× ×•×ª ×•×”××œ×¦×•×ª" ×œ×§×‘×œ×ª ×ª×•×‘× ×•×ª

**×œ×©××œ×•×ª × ×•×¡×¤×•×ª, ×¤× ×” ×œ××¤×ª×— ×”×‘×•×˜.**
        """
        await update.message.reply_text(help_text, parse_mode=ParseMode.MARKDOWN)
    
    def has_data(self, user_id: int) -> bool:
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚, ĞµÑÑ‚ÑŒ Ğ»Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñƒ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ"""
        if user_id not in self.user_data:
            return False
        data = self.user_data[user_id].get('data')
        return data is not None and isinstance(data, pd.DataFrame) and not data.empty
    
    async def handle_document(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """×˜×™×¤×•×œ ×‘×§×‘×¦×™× ×©×”×•×¢×œ×•"""
        user_id = update.effective_user.id
        document = update.message.document
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ĞµÑÑ‚ÑŒ Ğ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        if user_id not in self.user_data:
            self.user_data[user_id] = {'data': None, 'file_name': None, 'analysis_done': False}
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿ Ñ„Ğ°Ğ¹Ğ»Ğ°
        file_name = document.file_name
        file_extension = os.path.splitext(file_name)[1].lower()
        
        supported_formats = ['.csv', '.xlsx', '.xls']
        if file_extension not in supported_formats:
            await update.message.reply_text(
                f"âŒ ×¡×•×’ ×§×•×‘×¥ ×œ× × ×ª××š: {file_extension}\n\n×”×§×‘×¦×™× ×”× ×ª××›×™×: {', '.join(supported_formats)}"
            )
            return
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ñ„Ğ°Ğ¹Ğ»Ğ° (Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 50MB)
        max_size = 50 * 1024 * 1024  # 50MB
        if document.file_size > max_size:
            await update.message.reply_text(
                f"âŒ ×”×§×•×‘×¥ ×’×“×•×œ ××“×™: {document.file_size // (1024*1024)}MB\n\n×”×’×•×“×œ ×”××§×¡×™××œ×™: 50MB"
            )
            return
        
        await update.message.reply_text("ğŸ“ ×§×•×‘×¥ ×”×ª×§×‘×œ! ××¢×‘×“...")
        
        try:
            # Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ„Ğ°Ğ¹Ğ»
            file = await context.bot.get_file(document.file_id)
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¿ĞºÑƒ
            temp_dir = tempfile.mkdtemp()
            file_path = os.path.join(temp_dir, file_name)
            
            # Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ„Ğ°Ğ¹Ğ»
            await file.download_to_drive(file_path)
            
            # Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ Ñ„Ğ°Ğ¹Ğ»
            df = await self.read_data_file(file_path, file_extension)
            
            if df is not None and isinstance(df, pd.DataFrame) and not df.empty:
                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
                self.user_data[user_id].update({
                    'data': df,
                    'file_name': file_name,
                    'analysis_done': False
                })
                
                # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğµ
                rows, cols = df.shape
                await update.message.reply_text(
                    f"âœ… ×”×§×•×‘×¥ ×¢×•×‘×“ ×‘×”×¦×œ×—×”!\n\n"
                    f"ğŸ“Š ××™×“×¢ ×¢×œ ×”×§×•×‘×¥:\n"
                    f"â€¢ ×©×: {file_name}\n"
                    f"â€¢ ×©×•×¨×•×ª: {rows:,}\n"
                    f"â€¢ ×¢××•×“×•×ª: {cols}\n"
                    f"â€¢ ×’×•×“×œ: {document.file_size // 1024}KB\n\n"
                    f"×¢×›×©×™×• ××ª×” ×™×›×•×œ ×œ×‘×—×•×¨ '× ×™×ª×•×— × ×ª×•× ×™×', '×ª×¨×©×™××™×' ××• '×ª×•×‘× ×•×ª ×•×”××œ×¦×•×ª'!"
                )
                
                # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ñ€Ğ¾Ğº (ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾)
                preview = df.head(2).to_string(index=False, max_cols=3)
                if len(preview) > 1000:
                    preview = preview[:1000] + "..."
                await update.message.reply_text(f"ğŸ‘€ ×ª×¦×•×’×” ××§×“×™××”:\n```\n{preview}\n```", parse_mode=ParseMode.MARKDOWN)
                
            else:
                await update.message.reply_text("âŒ ×©×’×™××” ×‘×§×¨×™××ª ×”×§×•×‘×¥. ×× × ×•×“× ×©×”×§×•×‘×¥ ×ª×§×™×Ÿ ×•×œ× ×¨×™×§.")
            
        except Exception as e:
            logger.error(f"Error handling document: {e}")
            await update.message.reply_text("âŒ ×©×’×™××” ×‘×¢×™×‘×•×“ ×”×§×•×‘×¥. ×× × × ×¡×” ×©×•×‘.")
        
        finally:
            # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¿Ğ°Ğ¿ĞºÑƒ
            if 'temp_dir' in locals():
                shutil.rmtree(temp_dir, ignore_errors=True)
    
    async def read_data_file(self, file_path: str, file_extension: str):
        """×§×¨×™××ª ×§×•×‘×¥ × ×ª×•× ×™×"""
        try:
            if file_extension == '.csv':
                # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸
                encodings = ['utf-8', 'latin-1', 'cp1255', 'iso-8859-8']
                for encoding in encodings:
                    try:
                        df = pd.read_csv(file_path, encoding=encoding)
                        if isinstance(df, pd.DataFrame) and not df.empty:
                            return df
                    except UnicodeDecodeError:
                        continue
                return None
            
            elif file_extension in ['.xlsx', '.xls']:
                df = pd.read_excel(file_path)
                if isinstance(df, pd.DataFrame) and not df.empty:
                    return df
            
            return None
            
        except Exception as e:
            logger.error(f"Error reading file {file_path}: {e}")
            return None
    
    async def handle_text(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """×˜×™×¤×•×œ ×‘×”×•×“×¢×•×ª ×˜×§×¡×˜ ×¤×©×•×˜×•×ª"""
        user_id = update.effective_user.id
        text = update.message.text
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ĞµÑÑ‚ÑŒ Ğ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        if user_id not in self.user_data:
            self.user_data[user_id] = {'data': None, 'file_name': None, 'analysis_done': False}
        
        if text == 'ğŸ“Š × ×™×ª×•×— × ×ª×•× ×™×':
            await self.handle_analyze_data(update, context)
        
        elif text == 'ğŸ“ˆ ×ª×¨×©×™××™×':
            await self.handle_charts(update, context)
        
        elif text == 'ğŸ’¡ ×ª×•×‘× ×•×ª ×•×”××œ×¦×•×ª':
            await self.handle_insights(update, context)
        
        elif text == 'ğŸ“ ×”×¢×œ××ª ×§×•×‘×¥':
            await update.message.reply_text(
                "ğŸ“ ×©×œ×— ×œ×™ ×§×•×‘×¥ CSV ××• Excel ×›×“×™ ×œ×”×ª×—×™×œ!\n\n"
                "×× ×™ ×ª×•××š ×‘×§×‘×¦×™×:\n"
                "â€¢ CSV (.csv)\n"
                "â€¢ Excel (.xlsx, .xls)\n\n"
                "×’×•×“×œ ××§×¡×™××œ×™: 50MB"
            )
        
        elif text == 'â“ ×¢×–×¨×”':
            await self.help_command(update, context)
        
        else:
            await update.message.reply_text(
                "×œ× ×”×‘× ×ª×™ ××ª ×”×”×•×“×¢×” ×©×œ×š. ×× × ×”×©×ª××© ×‘×›×¤×ª×•×¨×™× ××• ×©×œ×— /help"
            )
    
    async def handle_analyze_data(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """×˜×™×¤×•×œ ×‘× ×™×ª×•×— × ×ª×•× ×™×"""
        user_id = update.effective_user.id
        
        if not self.has_data(user_id):
            await update.message.reply_text(
                "âŒ ××™×Ÿ × ×ª×•× ×™× ×œ× ×™×ª×•×—!\n\n"
                "×× × ×©×œ×— ×œ×™ ×§×•×‘×¥ CSV ××• Excel ×ª×—×™×œ×”."
            )
            return
        
        await update.message.reply_text("ğŸ” ×× ×ª×— × ×ª×•× ×™×...")
        
        try:
            df = self.user_data[user_id]['data']
            
            # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·
            analysis_text = f"ğŸ” **× ×™×ª×•×— ××¤×•×¨×˜: {self.user_data[user_id]['file_name']}**\n\n"
            
            # ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ
            rows, cols = df.shape
            analysis_text += f"ğŸ“Š **××™×“×¢ ×‘×¡×™×¡×™:**\n"
            analysis_text += f"â€¢ ××¡×¤×¨ ×©×•×¨×•×ª: {rows:,}\n"
            analysis_text += f"â€¢ ××¡×¤×¨ ×¢××•×“×•×ª: {cols}\n"
            analysis_text += f"â€¢ ×©× ×§×•×‘×¥: {self.user_data[user_id]['file_name']}\n\n"
            
            # Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ°Ñ…
            analysis_text += f"**×¢××•×“×•×ª ×•×˜×™×¤×•×¡×™ × ×ª×•× ×™×:**\n"
            for i, col in enumerate(df.columns, 1):
                col_type = str(df[col].dtype)
                null_count = df[col].isnull().sum()
                unique_count = df[col].nunique()
                analysis_text += f"{i}. {col} ({col_type})"
                if null_count > 0:
                    null_percentage = (null_count / len(df)) * 100
                    analysis_text += f" - {null_count} ×¢×¨×›×™× ×—×¡×¨×™× ({null_percentage:.1f}%)"
                analysis_text += f" - {unique_count} ×¢×¨×›×™× ×™×™×—×•×“×™×™×\n"
            
            # Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                analysis_text += f"\nğŸ“Š **×¡×˜×˜×™×¡×˜×™×§×” ××¡×¤×¨×™×ª ××¤×•×¨×˜×ª:**\n"
                for col in numeric_cols:
                    stats = df[col].describe()
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    analysis_text += f"\n**{col}:**\n"
                    analysis_text += f"â€¢ ×××•×¦×¢: {stats['mean']:.2f}\n"
                    analysis_text += f"â€¢ ×—×¦×™×•×Ÿ: {stats['50%']:.2f}\n"
                    analysis_text += f"â€¢ ×¡×˜×™×™×ª ×ª×§×Ÿ: {stats['std']:.2f}\n"
                    analysis_text += f"â€¢ ××™× ×™××•×: {stats['min']:.2f}\n"
                    analysis_text += f"â€¢ ××§×¡×™××•×: {stats['max']:.2f}\n"
                    analysis_text += f"â€¢ Q1: {Q1:.2f}\n"
                    analysis_text += f"â€¢ Q3: {Q3:.2f}\n"
            
            # ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº
            categorical_cols = df.select_dtypes(include=['object']).columns
            if len(categorical_cols) > 0:
                analysis_text += f"\n**× ×™×ª×•×— ×§×˜×’×•×¨×™×•×ª:**\n"
                for col in categorical_cols[:3]:  # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 3
                    value_counts = df[col].value_counts()
                    most_common = value_counts.head(3)
                    analysis_text += f"â€¢ {col}:\n"
                    for val, count in most_common.items():
                        percentage = (count / len(df)) * 100
                        analysis_text += f"  - {val}: {count} ({percentage:.1f}%)\n"
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹
            duplicates = df.duplicated().sum()
            if duplicates > 0:
                analysis_text += f"\n**âš ï¸ ××–×”×¨×•×ª:**\n"
                analysis_text += f"â€¢ × ××¦××• {duplicates} ×©×•×¨×•×ª ×›×¤×•×œ×•×ª\n"
            
            # ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
            total_cells = len(df) * len(df.columns)
            total_nulls = df.isnull().sum().sum()
            if total_nulls > 0:
                null_percentage = (total_nulls / total_cells) * 100
                analysis_text += f"\n**ğŸ” ××™×›×•×ª × ×ª×•× ×™×:**\n"
                analysis_text += f"â€¢ ×¢×¨×›×™× ×—×¡×¨×™×: {total_nulls:,} ({null_percentage:.1f}% ××”× ×ª×•× ×™×)\n"
                if null_percentage > 20:
                    analysis_text += f"  - âš ï¸ ××—×•×– ×’×‘×•×” ×©×œ ×¢×¨×›×™× ×—×¡×¨×™× - ×©×§×•×œ ×œ×‘×“×•×§ ××ª ××§×•×¨ ×”× ×ª×•× ×™×\n"
                elif null_percentage > 10:
                    analysis_text += f"  - âš ï¸ ××—×•×– ×‘×™× ×•× ×™ ×©×œ ×¢×¨×›×™× ×—×¡×¨×™× - ×™×™×ª×›×Ÿ ×©×™×™×“×¨×© ×˜×™×¤×•×œ\n"
                else:
                    analysis_text += f"  - âœ… ××—×•×– × ××•×š ×©×œ ×¢×¨×›×™× ×—×¡×¨×™× - × ×ª×•× ×™× ×‘××™×›×•×ª ×˜×•×‘×”\n"
            
            self.user_data[user_id]['analysis_done'] = True
            
            # Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ
            if len(analysis_text) > 4000:
                parts = [analysis_text[i:i+4000] for i in range(0, len(analysis_text), 4000)]
                for i, part in enumerate(parts):
                    if i == 0:
                        await update.message.reply_text(part, parse_mode=ParseMode.MARKDOWN)
                    else:
                        await update.message.reply_text(f"ğŸ“Š ×”××©×š ×”× ×™×ª×•×— (×—×œ×§ {i+1}):\n\n{part}", parse_mode=ParseMode.MARKDOWN)
            else:
                await update.message.reply_text(analysis_text, parse_mode=ParseMode.MARKDOWN)
            
            await update.message.reply_text(
                "âœ… ×”× ×™×ª×•×— ×”×•×©×œ×! ×¢×›×©×™×• ××ª×” ×™×›×•×œ ×œ×‘×—×•×¨ '×ª×¨×©×™××™×' ××• '×ª×•×‘× ×•×ª ×•×”××œ×¦×•×ª'."
            )
            
        except Exception as e:
            logger.error(f"Error analyzing data: {e}")
            await update.message.reply_text("âŒ ×©×’×™××” ×‘× ×™×ª×•×— ×”× ×ª×•× ×™×")
    
    async def handle_charts(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """×˜×™×¤×•×œ ×‘×ª×¨×©×™××™× - ×™×¦×™×¨×ª ×ª×¨×©×™××™× ××§×¦×•×¢×™×™× ×•××ª×§×“××™×"""
        user_id = update.effective_user.id
        
        if not self.has_data(user_id):
            await update.message.reply_text(
                "âŒ ××™×Ÿ × ×ª×•× ×™× ×œ×ª×¨×©×™××™×!\n\n"
                "×× × ×©×œ×— ×œ×™ ×§×•×‘×¥ CSV ××• Excel ×ª×—×™×œ×”."
            )
            return
        
        await update.message.reply_text("ğŸ“ˆ ×™×•×¦×¨ ×ª×¨×©×™××™× ××§×¦×•×¢×™×™×...")
        
        try:
            df = self.user_data[user_id]['data']
            chart_files = []
            chart_insights = {}
            chart_next_steps = {}
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ğ°Ğ¿ĞºÑƒ Ğ´Ğ»Ñ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ²
            temp_charts_dir = tempfile.mkdtemp()
            
            # ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° ÑÑ‚Ğ¸Ğ»Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¾Ğ²
            plt.style.use('seaborn-v0_8')
            sns.set_palette("husl")
            
            # 1. Ğ“Ğ¸ÑÑ‚Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ¾Ğ¼
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                for col in numeric_cols[:3]:  # ĞŸĞµÑ€Ğ²Ñ‹Ğµ 3 Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸
                    plt.figure(figsize=(12, 8))
                    
                    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ´Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸
                    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
                    
                    # Ğ“Ğ¸ÑÑ‚Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°
                    ax1.hist(df[col].dropna(), bins=30, alpha=0.7, color='skyblue', 
                            edgecolor='navy', linewidth=1.2)
                    ax1.set_title(f'×”×™×¡×˜×•×’×¨××” ×©×œ {col}', fontsize=16, fontweight='bold', pad=20)
                    ax1.set_xlabel(col, fontsize=12, fontweight='bold')
                    ax1.set_ylabel('×ª×“×™×¨×•×ª', fontsize=12, fontweight='bold')
                    ax1.grid(True, alpha=0.3, linestyle='--')
                    ax1.axvline(df[col].mean(), color='red', linestyle='--', linewidth=2, 
                               label=f'×××•×¦×¢: {df[col].mean():.2f}')
                    ax1.axvline(df[col].median(), color='green', linestyle='--', linewidth=2, 
                               label=f'×—×¦×™×•×Ÿ: {df[col].median():.2f}')
                    ax1.legend(fontsize=10)
                    
                    # Box plot
                    ax2.boxplot(df[col].dropna(), patch_artist=True, 
                               boxprops=dict(facecolor='lightblue', alpha=0.7),
                               medianprops=dict(color='red', linewidth=2))
                    ax2.set_title(f'Box Plot ×©×œ {col}', fontsize=14, fontweight='bold')
                    ax2.set_ylabel(col, fontsize=12, fontweight='bold')
                    ax2.grid(True, alpha=0.3, linestyle='--')
                    
                    plt.tight_layout()
                    chart_path = os.path.join(temp_charts_dir, f'histogram_box_{col}.png')
                    plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
                    plt.close()
                    chart_files.append(chart_path)

                    series = df[col].dropna()
                    if not series.empty:
                        mean_v = series.mean()
                        median_v = series.median()
                        std_v = series.std()
                        skew_v = series.skew()
                        q1 = series.quantile(0.25)
                        q3 = series.quantile(0.75)
                        iqr = q3 - q1
                        lower = q1 - 1.5 * iqr
                        upper = q3 + 1.5 * iqr
                        outliers = ((series < lower) | (series > upper)).sum()
                        out_pct = (outliers / len(series)) * 100.0
                        chart_insights[chart_path] = f"{col}: ×××•×¦×¢ {mean_v:.2f}, ×—×¦×™×•×Ÿ {median_v:.2f}, ×¡×˜×™×™×ª ×ª×§×Ÿ {std_v:.2f}, ×”×˜×™×” {skew_v:.2f}. ×—×¨×™×’×™×: {out_pct:.1f}%"
                        chart_next_steps[chart_path] = (
                            "××” ×”×œ××”:\n"
                            "â€¢ ×‘×“×™×§×ª ×—×¨×™×’×™× ×•×”×©×¤×¢×ª× ×¢×œ ×”××•×“×œ×™×\n"
                            "â€¢ ×× |×”×˜×™×”| ×’×‘×•×”×” â€” ×©×§×œ×• ×˜×¨× ×¡×¤×•×¨××¦×™×™×ª Log/Box-Cox\n"
                            "â€¢ ×”×©×•×•××ª ×”×”×ª×¤×œ×’×•×ª ×‘×™×Ÿ ×§×‘×•×¦×•×ª (A/B, ×¡×’×× ×˜×™×)"
                        )
            
            # 2. Ğ¡Ñ‚Ğ¾Ğ»Ğ±Ñ‡Ğ°Ñ‚Ñ‹Ğµ Ğ´Ğ¸Ğ°Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹ Ğ´Ğ»Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
            categorical_cols = df.select_dtypes(include=['object']).columns
            if len(categorical_cols) > 0:
                for col in categorical_cols[:2]:
                    series = df[col].dropna()
                    if series.empty:
                        continue
                    value_counts = series.value_counts()

                    # Detect high-cardinality columns and aggregate tail into 'Other'
                    unique_ratio = series.nunique() / len(series)
                    top_n = 10 if (unique_ratio > 0.3 or len(value_counts) > 12) else 15
                    value_counts = value_counts.sort_values(ascending=False)
                    others_sum = value_counts.iloc[top_n:].sum()
                    value_counts = value_counts.iloc[:top_n]
                    if others_sum > 0:
                        value_counts['××—×¨'] = others_sum

                    non_null_total = value_counts.sum()

                    plt.figure(figsize=(14, 8))
                    bars = plt.bar(range(len(value_counts)), value_counts.values,
                                 color=plt.cm.Set3(np.linspace(0, 1, len(value_counts))),
                                 alpha=0.8, edgecolor='black', linewidth=0.5)

                    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ñ‹
                    for i, (bar, value) in enumerate(zip(bars, value_counts.values)):
                        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01*max(value_counts.values),
                                f'{value}', ha='center', va='bottom', fontweight='bold', fontsize=10)

                    plt.title(f'×”×ª×¤×œ×’×•×ª {col}', fontsize=16, fontweight='bold', pad=20)
                    plt.xlabel(col, fontsize=12, fontweight='bold')
                    plt.ylabel('××¡×¤×¨', fontsize=12, fontweight='bold')
                    plt.xticks(range(len(value_counts)), value_counts.index, rotation=45, ha='right')
                    plt.grid(True, alpha=0.3, linestyle='--', axis='y')

                    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚ĞºĞ¸ (Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ĞµĞ½ÑƒĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ°)
                    for i, (bar, value) in enumerate(zip(bars, value_counts.values)):
                        percentage = (value / non_null_total) * 100 if non_null_total else 0
                        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,
                                f'{percentage:.1f}%', ha='center', va='center',
                                fontweight='bold', color='white', fontsize=9)

                    plt.tight_layout()
                    chart_path = os.path.join(temp_charts_dir, f'bar_chart_{col}.png')
                    plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
                    plt.close()
                    chart_files.append(chart_path)

                    top3 = value_counts.head(3)
                    coverage = (top3.sum() / non_null_total) * 100 if non_null_total else 0
                    dom = ', '.join([f"{k} ({v/non_null_total*100:.1f}%)" for k, v in top3.items()])
                    chart_insights[chart_path] = f"{col}: ×§×˜×’×•×¨×™×•×ª ××•×‘×™×œ×•×ª â€” {dom}. ×›×™×¡×•×™ ×˜×•×¤â€‘3: {coverage:.1f}%"
                    chart_next_steps[chart_path] = (
                        "××” ×”×œ××”:\n"
                        "â€¢ × ×™×ª×•×— ×¢×•××§ ×œ×¤×™ ×§×˜×’×•×¨×™×•×ª ××•×‘×™×œ×•×ª\n"
                        "â€¢ ×”××¨×ª ×§×˜×’×•×¨×™×•×ª ×“×œ×•×ª × ×ª×•× ×™× ×œ-'××—×¨'\n"
                        "â€¢ ×‘×“×™×§×ª ×§×©×¨ ×œ×™×¢×“×™ ×”××¨×”/×”×›× ×¡×”"
                    )
            
            # 3. ĞšĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğ° Ñ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ñ‹Ğ¼ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ¾Ğ¼
            if len(numeric_cols) > 1:
                plt.figure(figsize=(12, 10))
                correlation_matrix = df[numeric_cols].corr()
                
                # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¼Ğ°ÑĞºÑƒ Ğ´Ğ»Ñ Ğ²ĞµÑ€Ñ…Ğ½ĞµĞ³Ğ¾ Ñ‚Ñ€ĞµÑƒĞ³Ğ¾Ğ»ÑŒĞ½Ğ¸ĞºĞ°
                mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
                
                # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ heatmap
                sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdYlBu_r', 
                           center=0, square=True, linewidths=0.5, cbar_kws={"shrink": 0.8},
                           fmt='.3f', annot_kws={'size': 10, 'weight': 'bold'})
                
                plt.title('××˜×¨×™×¦×ª ×§×•×¨×œ×¦×™×” - Correlation Matrix', fontsize=16, fontweight='bold', pad=20)
                plt.tight_layout()
                
                chart_path = os.path.join(temp_charts_dir, 'correlation_matrix.png')
                plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
                plt.close()
                chart_files.append(chart_path)

                # Insights
                pairs = []
                cols_list = list(numeric_cols)
                for i in range(len(cols_list)):
                    for j in range(i+1, len(cols_list)):
                        val = correlation_matrix.loc[cols_list[i], cols_list[j]]
                        if not pd.isna(val):
                            pairs.append((cols_list[i], cols_list[j], float(val)))
                pairs.sort(key=lambda x: abs(x[2]), reverse=True)
                top_pairs = ', '.join([f"{a}â†”{b} ({c:.2f})" for a, b, c in pairs[:3]]) if pairs else "××™×Ÿ ×§×©×¨×™× ×—×–×§×™×"
                chart_insights[chart_path] = f"×–×•×’×•×ª ×§×•×¨×œ×¦×™×” ×‘×•×œ×˜×™×: {top_pairs}"
                chart_next_steps[chart_path] = (
                    "××” ×”×œ××”:\nâ€¢ ×‘×“×™×§×ª ×¨×’×¨×¡×™×” ×œ×–×•×’×•×ª ×—×–×§×™×\nâ€¢ ×˜×™×¤×•×œ ×‘××•×œ×˜×™×§×•×œ×™× ××¨×™×•×ª ×œ×¤× ×™ ML"
                )
            
            # 4. Scatter plot Ğ´Ğ»Ñ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº
            if len(numeric_cols) > 1:
                # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ scatter plot matrix
                fig, axes = plt.subplots(2, 2, figsize=(16, 12))
                fig.suptitle('Scatter Plots - ×’×¨×¤×™ ×¤×™×–×•×¨', fontsize=18, fontweight='bold')
                
                plot_count = 0
                for i in range(min(2, len(numeric_cols))):
                    for j in range(min(2, len(numeric_cols))):
                        if i != j and plot_count < 4:
                            row, col = plot_count // 2, plot_count % 2
                            axes[row, col].scatter(df[numeric_cols[i]], df[numeric_cols[j]], 
                                                 alpha=0.6, s=30, c='steelblue', edgecolors='black', linewidth=0.5)
                            axes[row, col].set_xlabel(numeric_cols[i], fontsize=10, fontweight='bold')
                            axes[row, col].set_ylabel(numeric_cols[j], fontsize=10, fontweight='bold')
                            axes[row, col].grid(True, alpha=0.3, linestyle='--')
                            
                            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ»Ğ¸Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµĞ½Ğ´Ğ°
                            z = np.polyfit(df[numeric_cols[i]].dropna(), df[numeric_cols[j]].dropna(), 1)
                            p = np.poly1d(z)
                            axes[row, col].plot(df[numeric_cols[i]], p(df[numeric_cols[i]]), 
                                              "r--", alpha=0.8, linewidth=2)
                            
                            plot_count += 1
                
                plt.tight_layout()
                chart_path = os.path.join(temp_charts_dir, 'scatter_matrix.png')
                plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
                plt.close()
                chart_files.append(chart_path)
                chart_insights[chart_path] = "×‘×“×§×• ×§×©×¨×™× ×œ×™× ×™××¨×™×™× ×•×§×œ××¡×˜×¨×™× ××¤×©×¨×™×™× ×‘×™×Ÿ ×–×•×’×•×ª ××©×ª× ×™×."
                chart_next_steps[chart_path] = "××” ×”×œ××”:\nâ€¢ ×”×•×¡×¤×ª ×§×•×•×™ ×¨×’×¨×¡×™×”\nâ€¢ × ×¡×™×•×Ÿ ×§×œ××¡×˜×¨×™×–×¦×™×” (KMeans)"
            
            # 5. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ (Distribution Analysis)
            if len(numeric_cols) > 0:
                fig, axes = plt.subplots(2, 2, figsize=(16, 12))
                fig.suptitle('× ×™×ª×•×— ×”×ª×¤×œ×’×•×ª - Distribution Analysis', fontsize=18, fontweight='bold')
                
                for i, col in enumerate(numeric_cols[:4]):
                    if i < 4:
                        row, col_idx = i // 2, i % 2
                        
                        # Histogram Ñ ĞºÑ€Ğ¸Ğ²Ğ¾Ğ¹ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
                        axes[row, col_idx].hist(df[col].dropna(), bins=25, density=True, alpha=0.7, 
                                               color='lightcoral', edgecolor='darkred', linewidth=1)
                        
                        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºÑ€Ğ¸Ğ²ÑƒÑ Ğ¿Ğ»Ğ¾Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸
                        data = df[col].dropna()
                        if len(data) > 0:
                            x = np.linspace(data.min(), data.max(), 100)
                            kde = stats.gaussian_kde(data)
                            axes[row, col_idx].plot(x, kde(x), 'b-', linewidth=2, label='KDE')
                        
                        axes[row, col_idx].set_title(f'{col}', fontsize=12, fontweight='bold')
                        axes[row, col_idx].set_xlabel('×¢×¨×š', fontsize=10)
                        axes[row, col_idx].set_ylabel('×¦×¤×™×¤×•×ª', fontsize=10)
                        axes[row, col_idx].grid(True, alpha=0.3, linestyle='--')
                        axes[row, col_idx].legend()
                
                plt.tight_layout()
                chart_path = os.path.join(temp_charts_dir, 'distribution_analysis.png')
                plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
                plt.close()
                chart_files.append(chart_path)

                skews = [abs(df[c].dropna().skew()) for c in numeric_cols[:4] if df[c].dropna().size > 0]
                if skews:
                    skew_avg = float(np.mean(skews))
                    chart_insights[chart_path] = f"×××•×¦×¢ ×”×˜×™×” ×‘××“×’×: {skew_avg:.2f}. ×¢×¨×›×™× ×’×‘×•×”×™× ××¢×™×“×™× ×¢×œ ×–× ×‘×•×ª ×›×‘×“×™×/××™-× ×•×¨××œ×™×•×ª."
                    chart_next_steps[chart_path] = "××” ×”×œ××”:\nâ€¢ ×©×§×™×œ×ª × ×¨××•×œ/×¡×˜× ×“×¨×˜×™×–×¦×™×”\nâ€¢ ×©×™××•×© ×‘××“×“×™× ×—×¡×™× ×™-×—×¨×™×’×™×"
            
            # 6. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ² (Outlier Analysis)
            if len(numeric_cols) > 0:
                fig, axes = plt.subplots(2, 2, figsize=(16, 12))
                fig.suptitle('× ×™×ª×•×— ×× ×•××œ×™×•×ª - Outlier Analysis', fontsize=18, fontweight='bold')
                
                for i, col in enumerate(numeric_cols[:4]):
                    if i < 4:
                        row, col_idx = i // 2, i % 2
                        
                        # Box plot Ñ Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸
                        bp = axes[row, col_idx].boxplot(df[col].dropna(), patch_artist=True,
                                                       boxprops=dict(facecolor='lightblue', alpha=0.7),
                                                       medianprops=dict(color='red', linewidth=2),
                                                       flierprops=dict(marker='o', markerfacecolor='red', 
                                                                      markersize=6, alpha=0.7))
                        
                        axes[row, col_idx].set_title(f'{col}', fontsize=12, fontweight='bold')
                        axes[row, col_idx].set_ylabel('×¢×¨×š', fontsize=10)
                        axes[row, col_idx].grid(True, alpha=0.3, linestyle='--')
                        
                        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ
                        stats_text = f'Q1: {df[col].quantile(0.25):.2f}\nQ3: {df[col].quantile(0.75):.2f}\nIQR: {df[col].quantile(0.75) - df[col].quantile(0.25):.2f}'
                        axes[row, col_idx].text(0.02, 0.98, stats_text, transform=axes[row, col_idx].transAxes,
                                               verticalalignment='top', fontsize=9, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
                
                plt.tight_layout()
                chart_path = os.path.join(temp_charts_dir, 'outlier_analysis.png')
                plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
                plt.close()
                chart_files.append(chart_path)

                outlier_rates = []
                for c in numeric_cols[:4]:
                    s = df[c].dropna()
                    if s.empty:
                        continue
                    q1, q3 = s.quantile(0.25), s.quantile(0.75)
                    iqr = q3 - q1
                    low, up = q1 - 1.5*iqr, q3 + 1.5*iqr
                    outlier_rates.append(((s < low) | (s > up)).mean()*100)
                if outlier_rates:
                    chart_insights[chart_path] = f"×©×™×¢×•×¨ ×—×¨×™×’×™× ×××•×¦×¢: {np.mean(outlier_rates):.1f}%."
                    chart_next_steps[chart_path] = "××” ×”×œ××”:\nâ€¢ ×˜×™×¤×•×œ ×‘×—×¨×™×’×™× (×—×™×ª×•×š/Winsorize)\nâ€¢ ×©×™××•×© ×‘××•×“×œ×™× ×—×¡×™× ×™-×—×¨×™×’×™×"
            
            # 7. Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ñ Ğ´Ğ°Ñ‚Ğ°Ğ¼Ğ¸)
            date_cols = [col for col in numeric_cols if any(keyword in col.lower() for keyword in ['date', 'time', 'year', 'month', 'day', '202', '201'])]
            if len(date_cols) > 0:
                for col in date_cols[:2]:
                    plt.figure(figsize=(14, 8))
                    
                    # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸
                    sorted_data = df.sort_values(col)
                    
                    plt.plot(sorted_data[col], sorted_data.index, 'o-', linewidth=2, markersize=4, 
                           color='purple', alpha=0.7, markerfacecolor='white', markeredgecolor='purple')
                    
                    plt.title(f'× ×™×ª×•×— ×˜×¨× ×“×™× - {col}', fontsize=16, fontweight='bold', pad=20)
                    plt.xlabel(col, fontsize=12, fontweight='bold')
                    plt.ylabel('××™× ×“×§×¡', fontsize=12, fontweight='bold')
                    plt.grid(True, alpha=0.3, linestyle='--')
                    
                    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ»Ğ¸Ğ½Ğ¸Ñ Ñ‚Ñ€ĞµĞ½Ğ´Ğ°
                    if len(sorted_data) > 1:
                        z = np.polyfit(range(len(sorted_data)), sorted_data[col], 1)
                        p = np.poly1d(z)
                        plt.plot(sorted_data[col], p(range(len(sorted_data))), "r--", alpha=0.8, linewidth=2, label='Trend')
                        plt.legend()
                    
                    plt.tight_layout()
                    chart_path = os.path.join(temp_charts_dir, f'trend_analysis_{col}.png')
                    plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
                    plt.close()
                    chart_files.append(chart_path)

                    try:
                        z = np.polyfit(range(len(sorted_data)), sorted_data[col], 1)
                        slope = float(z[0])
                        direction = '××’××ª ×¢×œ×™×”' if slope > 0 else '××’××ª ×™×¨×™×“×”' if slope < 0 else '×™×¦×™×‘'
                        chart_insights[chart_path] = f"{col}: {direction} (×©×™×¤×•×¢ {slope:.3f})."
                        chart_next_steps[chart_path] = "××” ×”×œ××”:\nâ€¢ ×‘×“×™×§×ª ×¢×•× ×ª×™×•×ª\nâ€¢ ×—×™×–×•×™ (ARIMA/Prophet)"
                    except Exception:
                        pass
            
            # 8. Ğ¡Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ² Ğ²Ğ¸Ğ´Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
            if len(numeric_cols) > 0:
                fig, ax = plt.subplots(figsize=(14, 8))
                ax.axis('tight')
                ax.axis('off')
                
                # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ ÑĞ¾ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¾Ğ¹
                stats_data = []
                for col in numeric_cols[:6]:  # ĞŸĞµÑ€Ğ²Ñ‹Ğµ 6 ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº
                    col_stats = df[col].describe()
                    stats_data.append([
                        col,
                        f"{col_stats['count']:.0f}",
                        f"{col_stats['mean']:.2f}",
                        f"{col_stats['std']:.2f}",
                        f"{col_stats['min']:.2f}",
                        f"{col_stats['25%']:.2f}",
                        f"{col_stats['50%']:.2f}",
                        f"{col_stats['75%']:.2f}",
                        f"{col_stats['max']:.2f}"
                    ])
                
                table = ax.table(cellText=stats_data,
                               colLabels=['×¢××•×“×”', '××¡×¤×¨', '×××•×¦×¢', '×¡×˜"×ª', '××™× ×™××•×', 'Q1', '×—×¦×™×•×Ÿ', 'Q3', '××§×¡×™××•×'],
                               cellLoc='center',
                               loc='center',
                               colWidths=[0.15, 0.1, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12])
                
                table.auto_set_font_size(False)
                table.set_fontsize(10)
                table.scale(1.2, 1.5)
                
                # Ğ¡Ñ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
                for i in range(len(stats_data) + 1):
                    for j in range(9):
                        if i == 0:  # Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸
                            table[(i, j)].set_facecolor('#4CAF50')
                            table[(i, j)].set_text_props(weight='bold', color='white')
                        else:  # Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ
                            if j == 0:  # ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº
                                table[(i, j)].set_facecolor('#E8F5E8')
                            else:  # Ğ§Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ
                                table[(i, j)].set_facecolor('#F8F9FA')
                
                plt.title('×¡×™×›×•× ×¡×˜×˜×™×¡×˜×™ - Statistical Summary', fontsize=16, fontweight='bold', pad=20)
                plt.tight_layout()
                
                chart_path = os.path.join(temp_charts_dir, 'statistical_summary.png')
                plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
                plt.close()
                chart_files.append(chart_path)

                try:
                    stds = {c: float(df[c].std()) for c in numeric_cols[:6]}
                    max_col = max(stds, key=stds.get)
                    chart_insights[chart_path] = f"×¡×˜×™×™×ª ×”×ª×§×Ÿ ×”×’×‘×•×”×” ×‘×™×•×ª×¨: {max_col} ({stds[max_col]:.2f}). ××¦×‘×™×¢ ×¢×œ ×©×•× ×•×ª ×’×‘×•×”×”."
                    chart_next_steps[chart_path] = "××” ×”×œ××”:\nâ€¢ × ×¨××•×œ ×œ×¤× ×™ ML\nâ€¢ ×¤×™×œ×•×— ×œ×”×§×˜× ×ª ×©×•× ×•×ª"
                except Exception:
                    pass
            
            # === 9. Ğ’Ğ¾Ñ€Ğ¾Ğ½ĞºĞ° (Funnel) ĞµÑĞ»Ğ¸ ÑƒĞ´Ğ°ĞµÑ‚ÑÑ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ğ¿Ñ‹ ===
            try:
                funnel_chart_created = False
                stage_keywords = ['stage','status','event','step','funnel','phase','state']
                ordered_stage_keywords = ['visit','view','signup','register','add_to_cart','checkout','purchase','pay','converted']
                
                # ĞšĞµĞ¹Ñ 1: Ğ¾Ğ´Ğ½Ğ° ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ° Ñ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼Ğ¸
                stage_col = None
                for col in df.select_dtypes(include=['object']).columns:
                    col_lower = col.lower()
                    if any(k in col_lower for k in stage_keywords):
                        if df[col].nunique() <= 12 and df[col].nunique() >= 3:
                            stage_col = col
                            break
                
                stage_counts = None
                stage_order = None
                
                if stage_col is not None:
                    counts = df[stage_col].value_counts()
                    # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ ÑƒĞ¿Ğ¾Ñ€ÑĞ´Ğ¾Ñ‡Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸, Ğ¸Ğ½Ğ°Ñ‡Ğµ Ğ¿Ğ¾ ÑƒĞ±Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹
                    order_map = {name: i for i, name in enumerate(ordered_stage_keywords)}
                    keys = list(counts.index)
                    if any(any(k in str(v).lower() for k in ordered_stage_keywords) for v in keys):
                        keys_sorted = sorted(keys, key=lambda v: order_map.get(str(v).lower(), 999))
                        stage_counts = counts[keys_sorted]
                        stage_order = keys_sorted
                    else:
                        stage_counts = counts
                        stage_order = list(counts.index)
                else:
                    # ĞšĞµĞ¹Ñ 2: Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ñ‹Ñ… ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº Ğ¿Ğ¾ ĞºĞ»ÑÑ‡Ğ°Ğ¼ ÑÑ‚Ğ°Ğ¿Ğ¾Ğ² (ÑÑƒĞ¼Ğ¼Ñ‹ Ğ¿Ğ¾ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ°Ğ¼)
                    cols = []
                    for key in ordered_stage_keywords:
                        for c in df.columns:
                            if key in c.lower():
                                cols.append(c)
                    cols = list(dict.fromkeys(cols))
                    if len(cols) >= 3:
                        stage_order = cols
                        totals = []
                        for c in cols:
                            series = pd.to_numeric(df[c], errors='coerce')
                            totals.append(np.nansum(series.values))
                        stage_counts = pd.Series(totals, index=cols)
                
                if stage_counts is not None and len(stage_counts) >= 3:
                    # Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ğ¼ ĞºÑ€Ğ°ÑĞ¸Ğ²ÑƒÑ Ğ²Ğ¾Ñ€Ğ¾Ğ½ĞºÑƒ
                    base = float(stage_counts.iloc[0]) if float(stage_counts.iloc[0]) != 0 else 1.0
                    conversions = [100.0]
                    for i in range(1, len(stage_counts)):
                        prev = float(stage_counts.iloc[i-1]) if float(stage_counts.iloc[i-1]) != 0 else 1.0
                        conv = (float(stage_counts.iloc[i]) / prev) * 100.0
                        conversions.append(conv)
                    
                    plt.figure(figsize=(12, 8))
                    vals = stage_counts.values.astype(float)
                    rel = vals / (vals[0] if vals[0] != 0 else 1.0)
                    colors = plt.cm.Blues(np.linspace(0.4, 0.9, len(vals)))
                    
                    y_pos = np.arange(len(stage_counts))[::-1]
                    plt.barh(y_pos, rel, color=colors, edgecolor='black', height=0.6)
                    
                    for i, (v, c) in enumerate(zip(vals, conversions)):
                        plt.text(rel[i] + 0.01, y_pos[i], f"{v:,.0f}  ({c:.1f}%)", va='center', fontsize=11, fontweight='bold')
                    
                    plt.yticks(y_pos, [str(s) for s in stage_counts.index])
                    plt.gca().invert_yaxis()
                    plt.title('×•×•×¨×•× ×§×” ×¢×¡×§×™×ª - Funnel Analysis', fontsize=16, fontweight='bold', pad=20)
                    plt.xlabel('×™×—×¡ ×œ×©×œ×‘ ×”×¨××©×•×Ÿ', fontsize=12, fontweight='bold')
                    plt.grid(True, axis='x', alpha=0.3, linestyle='--')
                    plt.tight_layout()
                    chart_path = os.path.join(temp_charts_dir, 'funnel_chart.png')
                    plt.savefig(chart_path, dpi=300, bbox_inches='tight', facecolor='white')
                    plt.close()
                    chart_files.append(chart_path)
                    funnel_chart_created = True
                    
                    # Ğ•ÑĞ»Ğ¸ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ â€” ÑĞ¾Ğ¾Ğ±Ñ‰Ğ¸Ğ¼ Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµÑ€ÑĞ¸ÑÑ…
                    try:
                        msg = ["ğŸ”» × ×™×ª×•×— ×•×•×¨×•× ×§×” (Funnel):"]
                        for i, name in enumerate(stage_counts.index):
                            msg.append(f"â€¢ {name}: {stage_counts.iloc[i]:,.0f}")
                            if i > 0:
                                msg.append(f"  â†³ ×”××¨×” ××©×œ×‘ ×§×•×“×: {conversions[i]:.1f}%")
                        # Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ğ¼ ÑÑ‚Ğ¾ ×›×ª×•×‘×™×ª ×œ×ª×¨×©×™×
                        overall = 0.0
                        try:
                            overall = (float(stage_counts.iloc[-1]) / float(stage_counts.iloc[0])) * 100.0 if float(stage_counts.iloc[0]) != 0 else 0.0
                        except Exception:
                            overall = 0.0
                        insight = [f"×”××¨×” ×›×•×œ×œ×ª: {overall:.1f}%"]
                        for j in range(1, len(stage_counts)):
                            insight.append(f"{stage_counts.index[j]}: {conversions[j]:.1f}% ××©×œ×‘ ×§×•×“×")
                        chart_insights[chart_path] = " | ".join(insight)
                        chart_next_steps[chart_path] = "××” ×”×œ××”:\nâ€¢ ××ª×¨ ××ª ×©×œ×‘ ×”× ×¤×™×œ×” ×•×˜×¤×œ ×‘×•\nâ€¢ ×œ×”×¨×™×¥ A/B ×¢×œ ×§×¨×™××•×ª ×œ×¤×¢×•×œ×”"
                    except:
                        pass
            except Exception as e:
                logging.exception("Funnel build failed")
            
            # 3. Funnel Chart
            funnel_cols = ['visit', 'signup', 'purchase']  # Example columns for funnel
            if all(col in df.columns for col in funnel_cols):
                funnel_data = df[funnel_cols].sum()
                plt.figure(figsize=(10, 6))
                plt.barh(funnel_cols, funnel_data, color=['#FF9999', '#66B3FF', '#99FF99'])
                plt.title('×ª×¨×©×™× ××©×¤×š', fontsize=16, fontweight='bold')
                plt.xlabel('×›××•×ª', fontsize=12, fontweight='bold')
                plt.ylabel('×©×œ×‘', fontsize=12, fontweight='bold')
                for index, value in enumerate(funnel_data):
                    plt.text(value, index, f'{value}', va='center', ha='right', fontweight='bold')
                plt.tight_layout()
                funnel_chart_path = os.path.join(temp_charts_dir, 'funnel_chart.png')
                plt.savefig(funnel_chart_path, dpi=300, bbox_inches='tight', facecolor='white')
                plt.close()
                chart_files.append(funnel_chart_path)
                try:
                    overall = (float(funnel_data.iloc[-1]) / float(funnel_data.iloc[0])) * 100.0 if float(funnel_data.iloc[0]) != 0 else 0.0
                except Exception:
                    overall = 0.0
                chart_insights[funnel_chart_path] = f"×”××¨×” ×›×•×œ×œ×ª: {overall:.1f}% | ×©×œ×‘×™×: {', '.join(funnel_cols)}"
                chart_next_steps[funnel_chart_path] = "××” ×”×œ××”:\nâ€¢ ×—×¤×©×• ×¦×•×•××¨×™ ×‘×§×‘×•×§ ×‘×™×Ÿ ×©×œ×‘×™×\nâ€¢ ×œ×”×¨×™×¥ A/B ×œ×©×™×¤×•×¨ CTA"
            
            # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸
            if chart_files:
                await update.message.reply_text(f"âœ… × ×•×¦×¨×• {len(chart_files)} ×ª×¨×©×™××™× ××§×¦×•×¢×™×™×!")
                
                # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
                chart_types = {
                    'histogram_box': 'ğŸ“Š ×”×™×¡×˜×•×’×¨××•×ª ×•-Box Plots',
                    'bar_chart': 'ğŸ“ˆ ×’×¨×¤×™× ×¢××•×“×•×ª',
                    'correlation_matrix': 'ğŸ”— ××˜×¨×™×¦×ª ×§×•×¨×œ×¦×™×”',
                    'scatter_matrix': 'ğŸ¯ ×’×¨×¤×™ ×¤×™×–×•×¨',
                    'distribution_analysis': 'ğŸ“ˆ × ×™×ª×•×— ×”×ª×¤×œ×’×•×ª',
                    'outlier_analysis': 'ğŸ” × ×™×ª×•×— ×× ×•××œ×™×•×ª',
                    'trend_analysis': 'ğŸ“ˆ × ×™×ª×•×— ×˜×¨× ×“×™×',
                    'statistical_summary': 'ğŸ“‹ ×¡×™×›×•× ×¡×˜×˜×™×¡×˜×™',
                    'funnel_chart': 'ğŸ”— ×•×•×¨×•× ×§×” ×¢×¡×§×™×ª'
                }
                
                for i, chart_file in enumerate(chart_files):
                    try:
                        with open(chart_file, 'rb') as img_file:
                            # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ° Ğ¿Ğ¾ Ğ¸Ğ¼ĞµĞ½Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ°
                            chart_type = "×ª×¨×©×™× ××§×¦×•×¢×™"
                            for key, value in chart_types.items():
                                if key in chart_file:
                                    chart_type = value
                                    break
                            
                            insight_text = chart_insights.get(chart_file, "")
                            caption = f"ğŸ“Š {chart_type}\n{insight_text}".strip()
                            if len(caption) > 900:
                                caption = caption[:900] + "â€¦"
                            await context.bot.send_photo(
                                chat_id=update.effective_chat.id,
                                photo=img_file,
                                caption=caption
                            )
                            next_steps = chart_next_steps.get(chart_file)
                            if next_steps:
                                await context.bot.send_message(chat_id=update.effective_chat.id, text=next_steps)
                    except Exception as e:
                        logger.error(f"Error sending chart {chart_file}: {e}")
                        await update.message.reply_text(f"âŒ ×©×’×™××” ×‘×©×œ×™×—×ª ×ª×¨×©×™× {i+1}")
                
                await update.message.reply_text(
                    "ğŸ‰ ×›×œ ×”×ª×¨×©×™××™× × ×©×œ×—×•! ×¢×›×©×™×• ×™×© ×œ×š × ×™×ª×•×— ×•×™×–×•××œ×™ ××§×™×£ ×•××§×¦×•×¢×™.\n\n"
                    "ğŸ’¡ **×¡×•×’×™ ×”×ª×¨×©×™××™× ×©× ×•×¦×¨×•:**\n"
                    "â€¢ ğŸ“Š ×”×™×¡×˜×•×’×¨××•×ª ×¢× Box Plots\n"
                    "â€¢ ğŸ“ˆ ×’×¨×¤×™× ×¢××•×“×•×ª ×¢× ××—×•×–×™×\n"
                    "â€¢ ğŸ”— ××˜×¨×™×¦×ª ×§×•×¨×œ×¦×™×” ××ª×§×“××ª\n"
                    "â€¢ ğŸ¯ ×’×¨×¤×™ ×¤×™×–×•×¨ ×¢× ×§×•×•×™ ×˜×¨× ×“\n"
                    "â€¢ ğŸ“ˆ × ×™×ª×•×— ×”×ª×¤×œ×’×•×ª ×¢× KDE\n"
                    "â€¢ ğŸ” × ×™×ª×•×— ×× ×•××œ×™×•×ª ××¤×•×¨×˜\n"
                    "â€¢ ğŸ“ˆ × ×™×ª×•×— ×˜×¨× ×“×™× ×–×× ×™×™×\n"
                    "â€¢ ğŸ“‹ ×¡×™×›×•× ×¡×˜×˜×™×¡×˜×™ ×‘×˜×‘×œ×”\n"
                    "â€¢ ğŸ”— ×•×•×¨×•× ×§×” ×¢×¡×§×™×ª\n\n"
                    "×¢×›×©×™×• ××ª×” ×™×›×•×œ ×œ×‘×—×•×¨ '×ª×•×‘× ×•×ª ×•×”××œ×¦×•×ª' ×œ×§×‘×œ×ª ×ª×•×‘× ×•×ª ×¢×¡×§×™×•×ª ××ª×§×“××•×ª!"
                )
            else:
                await update.message.reply_text("âŒ ×œ× × ×™×ª×Ÿ ×œ×™×¦×•×¨ ×ª×¨×©×™××™× ××”× ×ª×•× ×™× ×”× ×•×›×—×™×™×.")
            
        except Exception as e:
            logger.error(f"Error creating charts: {e}")
            await update.message.reply_text("âŒ ×©×’×™××” ×‘×™×¦×™×¨×ª ×”×ª×¨×©×™××™×")
        
        finally:
            # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹
            if 'temp_charts_dir' in locals():
                shutil.rmtree(temp_charts_dir, ignore_errors=True)
    
    async def handle_insights(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """×˜×™×¤×•×œ ×‘×ª×•×‘× ×•×ª ×•×”××œ×¦×•×ª"""
        user_id = update.effective_user.id
        
        if not self.has_data(user_id):
            await update.message.reply_text(
                "âŒ ××™×Ÿ × ×ª×•× ×™× ×œ×ª×•×‘× ×•×ª!\n\n"
                "×× × ×©×œ×— ×œ×™ ×§×•×‘×¥ ×ª×—×™×œ×”."
            )
            return
        
        await update.message.reply_text("ğŸ’¡ ×× ×ª×— ×ª×•×‘× ×•×ª ×•××›×™×Ÿ ×”××œ×¦×•×ª...")
        
        try:
            df = self.user_data[user_id]['data']
            insights_text = "ğŸ’¡ ×ª×•×‘× ×•×ª ××ª×§×“××•×ª ×•×”××œ×¦×•×ª:\n\n"
            
            # 1. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 1:
                insights_text += "**ğŸ”— × ×™×ª×•×— ×§×•×¨×œ×¦×™×•×ª:**\n"
                correlation_matrix = df[numeric_cols].corr()
                
                # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ñ‚Ğ¾Ğ¿-5 ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¹
                correlations = []
                for i in range(len(numeric_cols)):
                    for j in range(i+1, len(numeric_cols)):
                        col1, col2 = numeric_cols[i], numeric_cols[j]
                        corr_value = correlation_matrix.loc[col1, col2]
                        if not pd.isna(corr_value):
                            correlations.append((col1, col2, abs(corr_value)))
                
                # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ ÑĞ¸Ğ»Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞ»ÑÑ†Ğ¸Ğ¸
                correlations.sort(key=lambda x: x[2], reverse=True)
                
                for i, (col1, col2, corr_abs) in enumerate(correlations[:5]):
                    corr_value = correlation_matrix.loc[col1, col2]
                    insights_text += f"â€¢ {col1} â†” {col2}: {corr_value:.3f}\n"
                
                insights_text += "\n"
            
            # 2. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ¾Ğ²
            if len(numeric_cols) > 0:
                insights_text += "**ğŸ” ×–×™×”×•×™ ×× ×•××œ×™×•×ª:**\n"
                for col in numeric_cols[:3]:
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                    
                    if len(outliers) > 0:
                        outlier_percentage = (len(outliers) / len(df)) * 100
                        insights_text += f"â€¢ ×‘-{col}: × ××¦××• {len(outliers)} ×¢×¨×›×™× ×—×¨×™×’×™× ({outlier_percentage:.1f}%)\n"
                        insights_text += f"  - ×˜×•×•×— ×ª×§×™×Ÿ: {lower_bound:.2f} ×¢×“ {upper_bound:.2f}\n"
                        if outlier_percentage > 10:
                            insights_text += f"  - âš ï¸ ××—×•×– ×’×‘×•×” ×©×œ ×× ×•××œ×™×•×ª - ×™×™×ª×›×Ÿ ×©×™×™×“×¨×© ×˜×™×¤×•×œ\n"
                    else:
                        insights_text += f"â€¢ ×‘-{col}: ××™×Ÿ ×¢×¨×›×™× ×—×¨×™×’×™×\n"
                
                insights_text += "\n"
            
            # 3. ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ
            if len(numeric_cols) > 0:
                insights_text += "**ğŸ“Š × ×™×ª×•×— ×”×ª×¤×œ×’×•×ª:**\n"
                for col in numeric_cols[:2]:
                    skewness = df[col].skew()
                    if abs(skewness) > 1:
                        distribution_type = "××•×˜×” ×××•×“" if abs(skewness) > 2 else "××•×˜×”"
                        direction = "×™××™× ×”" if skewness > 0 else "×©×××œ×”"
                        insights_text += f"â€¢ {col}: ×”×ª×¤×œ×’×•×ª {distribution_type} {direction} (skewness: {skewness:.2f})\n"
                        if abs(skewness) > 2:
                            insights_text += f"  - âš ï¸ ×”×ª×¤×œ×’×•×ª ×××•×“ ××•×˜×” - ×©×§×•×œ ×œ×‘×“×•×§ ××ª ××§×•×¨ ×”× ×ª×•× ×™×\n"
                    else:
                        insights_text += f"â€¢ {col}: ×”×ª×¤×œ×’×•×ª × ×•×¨××œ×™×ª ×™×—×¡×™×ª (skewness: {skewness:.2f})\n"
                
                insights_text += "\n"
            
            # 4. ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
            categorical_cols = df.select_dtypes(include=['object']).columns
            if len(categorical_cols) > 0:
                insights_text += "**ğŸ·ï¸ × ×™×ª×•×— ×§×˜×’×•×¨×™×•×ª:**\n"
                for col in categorical_cols[:2]:
                    value_counts = df[col].value_counts()
                    diversity = len(value_counts) / len(df)
                    
                    if diversity > 0.8:
                        insights_text += f"â€¢ {col}: ××’×•×•×Ÿ ×’×‘×•×” ×××•×“ - {len(value_counts)} ×¢×¨×›×™× ×™×™×—×•×“×™×™×\n"
                        insights_text += f"  - ğŸ’¡ ××ª××™× ×œ× ×™×ª×•×— ×“×¤×•×¡×™× ××ª×§×“××™×\n"
                    elif diversity > 0.5:
                        insights_text += f"â€¢ {col}: ××’×•×•×Ÿ ×‘×™× ×•× ×™ - {len(value_counts)} ×¢×¨×›×™× ×™×™×—×•×“×™×™×\n"
                        insights_text += f"  - ğŸ’¡ ××ª××™× ×œ× ×™×ª×•×— ×¡×’×× ×˜×™\n"
                    else:
                        insights_text += f"â€¢ {col}: ××’×•×•×Ÿ × ××•×š - {len(value_counts)} ×¢×¨×›×™× ×™×™×—×•×“×™×™×\n"
                        insights_text += f"  - ğŸ’¡ ××ª××™× ×œ× ×™×ª×•×— ×”×©×•×•××ª×™\n"
                    
                    # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ´Ğ¾Ğ¼Ğ¸Ğ½Ğ¸Ñ€ÑƒÑÑ‰ÑƒÑ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ
                    dominant = value_counts.iloc[0]
                    dominant_percentage = (dominant / len(df)) * 100
                    if dominant_percentage > 50:
                        insights_text += f"  - ×§×˜×’×•×¨×™×” ×“×•××™× × ×˜×™×ª: {value_counts.index[0]} ({dominant_percentage:.1f}%)\n"
                        if dominant_percentage > 80:
                            insights_text += f"    âš ï¸ ×“×•××™× × ×˜×™×•×ª ×’×‘×•×”×” ×××•×“ - ×™×™×ª×›×Ÿ ×‘×¢×™×” ×‘××™×¡×•×£ × ×ª×•× ×™×\n"
                
                insights_text += "\n"
            
            # 5. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
            insights_text += "**ğŸ’¡ ×”××œ×¦×•×ª ×œ×©×™×¤×•×¨ ×”× ×ª×•× ×™×:**\n"
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ
            total_nulls = df.isnull().sum().sum()
            total_cells = len(df) * len(df.columns)
            if total_nulls > 0:
                null_percentage = (total_nulls / total_cells) * 100
                insights_text += f"â€¢ ×¢×¨×›×™× ×—×¡×¨×™×: {total_nulls:,} ({null_percentage:.1f}% ××”× ×ª×•× ×™×)\n"
                if null_percentage > 20:
                    insights_text += f"  - âš ï¸ ××—×•×– ×’×‘×•×” - ×‘×“×•×§ ××ª ××§×•×¨ ×”× ×ª×•× ×™× ××• ×”×©×ª××© ×‘×©×™×˜×•×ª ×”×©×œ××” ××ª×§×“××•×ª\n"
                elif null_percentage > 10:
                    insights_text += f"  - âš ï¸ ××—×•×– ×‘×™× ×•× ×™ - ×©×§×•×œ ×”×©×œ××” ×‘×××¦×¢×•×ª ×××•×¦×¢ ××• ×—×¦×™×•×Ÿ\n"
                else:
                    insights_text += f"  - âœ… ××—×•×– × ××•×š - × ×ª×•× ×™× ×‘××™×›×•×ª ×˜×•×‘×”\n"
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹
            duplicates = df.duplicated().sum()
            if duplicates > 0:
                duplicate_percentage = (duplicates / len(df)) * 100
                insights_text += f"â€¢ ×©×•×¨×•×ª ×›×¤×•×œ×•×ª: {duplicates:,} ({duplicate_percentage:.1f}%)\n"
                insights_text += f"  - ×”××œ×¦×”: ×”×¡×¨ ×›×¤×™×œ×•×™×•×ª ×œ×¤× ×™ ×”× ×™×ª×•×—\n"
            
            # Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
            conversion_recommendations = []
            for col in df.columns:
                if df[col].dtype == 'object':
                    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ»Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ñ‡Ğ¸ÑĞ»Ğ¾Ğ²Ğ¾Ğ¹ Ñ‚Ğ¸Ğ¿
                    try:
                        pd.to_numeric(df[col], errors='raise')
                        conversion_recommendations.append(col)
                    except:
                        pass
            
            if conversion_recommendations:
                insights_text += f"â€¢ ×¢××•×“×•×ª ×œ×”××¨×” ××¡×¤×¨×™×ª: {', '.join(conversion_recommendations)}\n"
                insights_text += f"  - ×”××œ×¦×”: ×”××¨ ×œ×˜×™×¤×•×¡ ××¡×¤×¨×™ ×œ× ×™×ª×•×— ××ª×§×“× ×™×•×ª×¨\n"
            
            insights_text += "\n"
            
            # 6. Ğ‘Ğ¸Ğ·Ğ½ĞµÑ-Ğ¸Ğ½ÑĞ°Ğ¹Ñ‚Ñ‹
            insights_text += "**ğŸš€ ×ª×•×‘× ×•×ª ×¢×¡×§×™×•×ª:**\n"
            
            if len(numeric_cols) > 0:
                # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒÑ
                max_var_col = numeric_cols[0]
                max_variance = df[max_var_col].var()
                for col in numeric_cols:
                    if df[col].var() > max_variance:
                        max_variance = df[col].var()
                        max_var_col = col
                
                insights_text += f"â€¢ ×”×¢××•×“×” {max_var_col} ××¨××” ××ª ×”×©×•× ×•×ª ×”×’×‘×•×”×” ×‘×™×•×ª×¨\n"
                insights_text += f"  - ×–×” ×¢×©×•×™ ×œ×”×¦×‘×™×¢ ×¢×œ ×”×–×“×× ×•×™×•×ª ××• ×¡×™×›×•× ×™× ×¢×¡×§×™×™×\n"
                
                # ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‚Ñ€ĞµĞ½Ğ´Ğ¾Ğ² (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸)
                date_cols = [col for col in numeric_cols if any(keyword in col.lower() for keyword in ['date', 'time', 'year', 'month', 'day'])]
                if date_cols:
                    insights_text += f"â€¢ ×¢××•×“×•×ª ×–××Ÿ ×–×•×”×•: {', '.join(date_cols)}\n"
                    insights_text += f"  - ğŸ’¡ ××ª××™× ×œ× ×™×ª×•×— ×˜×¨× ×“×™× ×•×–××Ÿ\n"
            
            if len(categorical_cols) > 0:
                # ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸
                for col in categorical_cols[:1]:
                    if df[col].nunique() <= 10:  # ĞĞµ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹
                        insights_text += f"â€¢ {col} ××ª××™× ×œ× ×™×ª×•×— ×¡×’×× ×˜×™\n"
                        insights_text += f"  - ğŸ’¡ ×©×§×•×œ ×œ× ×ª×— ×‘×™×¦×•×¢×™× ×œ×¤×™ ×§×˜×’×•×¨×™×•×ª\n"
                        insights_text += f"  - ğŸ’¡ ××ª××™× ×œ×™×¦×™×¨×ª ×“×©×‘×•×¨×“×™× ×¢×¡×§×™×™×\n"
            
            # 7. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¼Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ñƒ
            insights_text += "\n**ğŸ¯ ×”××œ×¦×•×ª ×œ× ×™×ª×•×— × ×•×¡×£:**\n"
            if len(numeric_cols) > 1:
                insights_text += "â€¢ × ×™×ª×•×— ×¨×’×¨×¡×™×” ×œ×–×™×”×•×™ ×’×•×¨××™× ××©×¤×™×¢×™×\n"
                insights_text += "â€¢ × ×™×ª×•×— ××©×›×•×œ×•×ª (Clustering) ×œ×–×™×”×•×™ ×“×¤×•×¡×™×\n"
            if len(categorical_cols) > 0:
                insights_text += "â€¢ × ×™×ª×•×— ANOVA ×œ×”×©×•×•××” ×‘×™×Ÿ ×§×‘×•×¦×•×ª\n"
                insights_text += "â€¢ × ×™×ª×•×— Chi-Square ×œ×‘×“×™×§×ª ×§×©×¨×™×\n"
            
            # === × ×™×ª×•×— ML (×¨×’×¨×¡×™×”/×¡×™×•×•×’ + ×§×œ××¡×˜×¨×™× ×’) ===
            try:
                numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
                ml_msgs = []
                if len(numeric_cols) >= 2:
                    clean = df[numeric_cols].dropna()
                    if len(clean) >= 30:
                        # ×™×¢×“ ×œ×¨×’×¨×¡×™×”: × × ×¡×” ×œ×–×”×•×ª ×œ×¤×™ ×©×, ××—×¨×ª × ×‘×—×¨ ×‘×¢×œ ×©×•× ×•×ª ×’×‘×•×”×”
                        target_candidates = [c for c in clean.columns if any(k in c.lower() for k in ['target','label','y','revenue','sales','price','amount'])]
                        y_col = target_candidates[0] if target_candidates else clean.var().sort_values(ascending=False).index[0]
                        X_cols = [c for c in clean.columns if c != y_col]
                        X = clean[X_cols].values
                        y = clean[y_col].values
                        
                        scaler = StandardScaler()
                        Xs = scaler.fit_transform(X)
                        
                        X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.25, random_state=42)
                        
                        # ×¨×’×¨×¡×™×” ×™×¢×¨ ××§×¨××™
                        rf = RandomForestRegressor(n_estimators=120, random_state=42)
                        rf.fit(X_train, y_train)
                        r2 = r2_score(y_test, rf.predict(X_test))
                        importances = rf.feature_importances_
                        top_idx = np.argsort(importances)[::-1][:5]
                        top_feats = ", ".join([f"{X_cols[i]} ({importances[i]:.3f})" for i in top_idx])
                        ml_msgs.append(f"â€¢ ×¨×’×¨×¡×™×” (×™×¢×“: {y_col}) â€” RÂ²={r2:.3f}. ×ª×›×•× ×•×ª ××•×‘×™×œ×•×ª: {top_feats}")
                        
                        # ×§×œ××¡×˜×¨×™× ×’ KMeans
                        k = 3
                        km = KMeans(n_clusters=k, n_init=10, random_state=42)
                        labels = km.fit_predict(Xs)
                        sizes = pd.Series(labels).value_counts().sort_index().tolist()
                        ml_msgs.append(f"â€¢ ×§×œ××¡×˜×¨×™× ×’ KMeans (k={k}) â€” ×’×“×œ×™×: {sizes}")
                if ml_msgs:
                    insights_text += "\n".join(["ğŸ”¬ × ×™×ª×•×— ML:"] + ml_msgs) + "\n\n"
            except Exception:
                logging.exception("ML analysis failed")
            
            # === A/B Testing ===
            try:
                ab_msgs = []
                # ×¢××•×“×ª ×§×‘×•×¦×”
                group_col = None
                for col in df.columns:
                    if df[col].nunique() == 2 and any(k in col.lower() for k in ['group','variant','test','bucket','arm','exposure','experiment']):
                        group_col = col
                        break
                if group_col is None:
                    # × ×¡×” ×œ××¦×•× ×“×•-×¢×¨×›×™×ª ×›×œ×œ×™×ª
                    for col in df.select_dtypes(include=['object']).columns:
                        if df[col].nunique() == 2:
                            group_col = col
                            break
                if group_col is not None:
                    # ×ª×•×¦× ×“×•-×¢×¨×›×™ ×œ×”××¨×”
                    bin_cols = []
                    for col in df.columns:
                        if col == group_col:
                            continue
                        series = pd.to_numeric(df[col], errors='coerce')
                        # × ×—×©×‘ ×× 0/1 ××• ×©×™×¢×•×¨ 0..1
                        unique_vals = pd.Series(series.dropna().unique())
                        if len(unique_vals) > 0 and unique_vals.isin([0,1]).all():
                            bin_cols.append(col)
                    
                    if bin_cols:
                        outcome = bin_cols[0]
                        g_vals = df[group_col].dropna().unique().tolist()
                        if len(g_vals) == 2:
                            g1, g2 = g_vals[0], g_vals[1]
                            g1_mask = df[group_col] == g1
                            g2_mask = df[group_col] == g2
                            x1 = pd.to_numeric(df.loc[g1_mask, outcome], errors='coerce')
                            x2 = pd.to_numeric(df.loc[g2_mask, outcome], errors='coerce')
                            x1 = x1.dropna()
                            x2 = x2.dropna()
                            if len(x1) > 10 and len(x2) > 10:
                                p1 = x1.mean()
                                p2 = x2.mean()
                                n1 = len(x1)
                                n2 = len(x2)
                                p_pool = (x1.sum() + x2.sum()) / (n1 + n2)
                                se = float(np.sqrt(p_pool * (1 - p_pool) * (1.0/n1 + 1.0/n2)))
                                se = se if se != 0 else 1e-9
                                z = (p1 - p2) / se
                                p_val = 2 * (1 - stats.norm.cdf(abs(z)))
                                ab_msgs.append(f"â€¢ ××‘×—×Ÿ ×©×™×¢×•×¨×™× (×§×‘×•×¦×” {g1} ××•×œ {g2}, ×ª×•×¦× {outcome}) â€” p={p_val:.4f}, ×”××¨×”: {p1:.3f} ××•×œ {p2:.3f}")
                    else:
                        # ×ª×•×¦× ×¨×¦×™×£ â€” t-test
                        metric_col = None
                        cand = [c for c in df.select_dtypes(include=[np.number]).columns if c != group_col]
                        if cand:
                            metric_col = cand[0]
                            g_vals = df[group_col].dropna().unique().tolist()
                            if len(g_vals) == 2:
                                g1, g2 = g_vals[0], g_vals[1]
                                x1 = pd.to_numeric(df.loc[df[group_col]==g1, metric_col], errors='coerce').dropna()
                                x2 = pd.to_numeric(df.loc[df[group_col]==g2, metric_col], errors='coerce').dropna()
                                if len(x1) > 10 and len(x2) > 10:
                                    t_stat, p_val = stats.ttest_ind(x1, x2, equal_var=False)
                                    ab_msgs.append(f"â€¢ t-test (××“×“ {metric_col}, {g1} ××•×œ {g2}) â€” p={p_val:.4f}, ×××•×¦×¢×™×: {x1.mean():.2f} ××•×œ {x2.mean():.2f}")
                if ab_msgs:
                    insights_text += "\n".join(["ğŸ§ª A/B Testing:"] + ab_msgs) + "\n\n"
            except Exception:
                logging.exception("AB analysis failed")
            
            # Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ
            if len(insights_text) > 4000:
                parts = [insights_text[i:i+4000] for i in range(0, len(insights_text), 4000)]
                for i, part in enumerate(parts):
                    if i == 0:
                        await update.message.reply_text(part, parse_mode=ParseMode.MARKDOWN)
                    else:
                        await update.message.reply_text(f"ğŸ’¡ ×”××©×š ×”×ª×•×‘× ×•×ª (×—×œ×§ {i+1}):\n\n{part}", parse_mode=ParseMode.MARKDOWN)
            else:
                await update.message.reply_text(insights_text, parse_mode=ParseMode.MARKDOWN)
            
            await update.message.reply_text(
                "ğŸ¯ ×”×ª×•×‘× ×•×ª ×•×”×”××œ×¦×•×ª ×”×•×©×œ××•! ×¢×›×©×™×• ×™×© ×œ×š ×ª××•× ×” ××œ××” ×©×œ ×”× ×ª×•× ×™× ×©×œ×š."
            )
            
        except Exception as e:
            logger.error(f"Error generating insights: {e}")
            await update.message.reply_text("âŒ ×©×’×™××” ×‘×™×¦×™×¨×ª ×”×ª×•×‘× ×•×ª")
    
    def run(self):
        """×”×¤×¢×œ×ª ×”×‘×•×˜"""
        logger.info("Starting Simple Hebrew Bot...")
        self.application.run_polling()

def main():
    """×”×¤×•× ×§×¦×™×” ×”×¨××©×™×ª"""
    BOT_TOKEN = "REDACTED"
    
    try:
        # Avoid Unicode in Windows CP1251 console
        print("Starting Simple Hebrew Bot...")
        bot = SimpleHebrewBot(BOT_TOKEN)
        print("Bot created successfully!")
        print("Starting bot...")
        print("Now find the bot in Telegram and send /start")
        print("You can now upload CSV/Excel files!")
        print("Full analytics, charts, and insights are now available!")
        
        bot.run()
        
    except Exception as e:
        print(f"Error starting bot: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
